---
title: "Online Regime Change Detection"
author: "Kyle Pelham, kyleplhm@gmail.com"
date: "2023-03-29"
output: 
  html_document: 
    theme: cerulean
    df_print: kable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

```{r}
library(tidyverse)
library(here)
library(ggplot2)
library(onlineBcp)
library(lubridate)
library(plotly)
library(gt)
```

# Introduction

This analysis is my contribution to a larger Cal Poly MSBA capstone project in support of a large North American Utilities company. Our group was tasked with detecting significant changes in constraint flow beyond normal seasonality also known as **regime changes.** This model leverages an R package that uses Bayesian probabilities to detect changes in time-series data in real-time( a.k.a "online").

The data used in this demonstration is an extract of historical provided by the client for a single constraint. Any geographical labels or names have been removed to maintain confidentiality. Only the knitted html is available at the request of the client.

#### What is a constraint? What is constraint flow? Why is this important?

In the context of a utilities company, constraints refer to limitations in the power grid, impacting its efficiency and reliability. These constraints can arise from transmission line capacity, generation limitations, network topology, and regulatory or market restrictions. When combined with the term "constraint flow," it refers to the flow of electricity through a constrained part of the power grid, which can be influenced by these factors. Power companies and grid operators work to mitigate these constraints, ensuring stable and cost-effective electricity delivery.

Having a method to detect large changes in constraint flow can allow the utilities company to take proactive actions to maintain stability within the power grid instead of reacting when things begin to breakdown. If this change persists, then it is considered a regime change and the utilities company can work towards stabilizing the higher demand for power.

#### Approach:

1.  Import & clean the constraint data

2.  Exploratory analysis

3.  Create baseline with OnlineBcp package

4.  Perform post-processing on OnlineBcp output to determine regime change

5.  Visualize results and suggest appropriate actions

# Import & Clean the Constraint Data

#### Data Dictionary:

-   DateTime: Date and time of observation. Each observation is one hour

-   OpYear: Year observation occurs in

-   OpMonth: Year + month observation occurs in

-   constraint_flow: Flow of electricity within constraint in megawatt hours (MWh)

-   rate_b: The intended upper limit on constraint flow at that given time

-   constraint_flow_ratio: Ratio of constraint flow over rate_b (e.g., constraint_flow/rate_b)

-   shadow_price: cost of intervening when the power constraint is near its limit. e.g., constraint_flow is approaching rate_b

#### Data Cleaning

Reading in the data and using the summary() function shows us that we have a consistent number of NA values across the rate_b, constraint_flow, and constraint_flow_ratio. This is due to those rows being recorded only once per day despite having rows for every hour of the day.

```{r}
df  <- read_csv(here('Data','constraint.csv'), guess_max = 100000)

summary(df)

```

We can safely remove those rows by filtering where constraint_flow is not NA. DateTime is also converted from a string to a DateTime format.

```{r}
df = df %>%
    filter(
      !(is.na(constraint_flow))
    ) 

df$DateTime = as.POSIXct(df$DateTime, format="%m/%d/%Y %H:%M")

summary(df)
```

Using the summary() function again we can see that those rows have been dropped. However, there are still many NA values under shadow_price. This is due to shadow price only being recorded when constraint_flow is near its capacity (rate_b), requiring manual intervention. When this happens, it is referred to as a 'binding_hour'.

This is not necessary for this model, but it is important to track for our client. The dummy variable binding_hour was created to determine when this sequence of events happens. NA rows in shadow_price were also replaced with 0.

Finally, all NA values have been removed and each column is in the correct format.

```{r}

df = df %>%
    mutate(
      binding_hour = factor(case_when(
        is.na(shadow_price) ~ 0,
        TRUE ~ 1
      )),
      shadow_price = case_when(
        is.na(shadow_price) ~ 0,
        TRUE ~ shadow_price
      )
    )

summary(df)

head(df, n=10)
```

# Exploring the Constraint Data

#### Plotting the Whole Dataset

Since this is all historical data, we will need to simulate how this would work in real time. First, we need to find a regime change in the past that we will try and detect. Lets look at a plot of the constraint time-series data with constraint_flow on the y-axis and DateTime on the x axis.

In the graph below, the blue line is the constraint_flow variable and the orange line is rate_b. Remember, rate_b is the upper threshold of constraint_flow before problems arise.

```{r}
df %>%
  ggplot() + geom_line(aes(x = DateTime, y = constraint_flow), color = 'blue') + geom_line(aes(x = DateTime, y = rate_b), color = 'orange') + labs(x = 'Date', y = 'Constraint Flow')
```

Right at the start of 2022, there appears to be a huge shift in constraint flow where it is riding right up to and beyond the rate_b limit. This is indicative of a regime change and is likely costing our client money to keep the grid stable.

This is a good example of what this model is looking to detect, so this is the time period that will be simulated. We don't need all of the data prior to the expected change so we will progress with only data from 2021 and beyond.

#### Filtering out Dates Before 2021

```{r}
df = df %>%
  filter(
    DateTime >= '2021-01-01'
  )

df %>%
  ggplot() + geom_line(aes(x = DateTime, y = constraint_flow), color = 'blue') + geom_line(aes(x = DateTime, y = rate_b), color = 'orange') + labs(x = 'Date', y = 'Constraint Flow')
```

# Bayesian Online Change Point Detection Workflow

The onlineBcp package is using Bayesian probabilities to determine if an observation is a change based on previous observations. In other words, you can think of this function processing the time-series data from left to right only. It is not using future data to determine change points like some other methods may use. This is what allows this method to detect changes in real-time.

#### Running the Model

For this section, the whole time-series data set will be passed in to showcase how it is working under the hood. In the real world, new data would be appended periodically and the function would be ran after each append to looking for new changes.

The constraint_flow variable is passed into the online_cp function and the th_cp parameter is set to 0.98. Th_cp is the uper threshold of probability for the data point to be classified as a change point. In other words, only the points that have a greater than or equal to 98% chance of being a change point are classified. This allows the model to highlight only very large changes that are indicative of regime change.

```{r, results='hide'}
# Run online_cp function on df with threshold set to 0.98
ocp <- online_cp(df$constraint_flow, th_cp = 0.98)
# Assigning results to ocp_summary
ocp_summary <- summary(ocp)
```

Plotting the output of the function shows us where each change point is occurring and draws a line through the mean of each resulting segment.

```{r}
# Show onlineBcp plot
plot(ocp_summary)
```

The output also provides a table of summary statistics for each segment, including the rows where they start and end. Running post-processing on this table is what allows us to determine where the especially large changes are occurring.

```{r}
# Show results table
ocp_summary$result[[2]]
```

#### Post-processing

To compare the segments, Cohen's D is calculated for each segment. The higher the D value, the more different they are to the previous segment.

```{r}
# Calculating Cohen's D and time in Months for each segment
table <- as.data.frame(ocp_summary$result[[2]])
 
table <- table %>%
    mutate(
      Months = (as.numeric(as.duration(df$DateTime[table$end] -  df$DateTime[table$begin]) / ddays(1)))/30.44,
      CohenD = (abs(lag(mean) - mean)) / sqrt((SD^2 + lag(SD)^2)/2)
    )
 
table
```

To find changes indicative of regime change, we are looking for changes that are greater than or equal to a D value of 2. If it meets that requirement, a new variable titled 'LargeChange' will be assigned to TRUE. By filtering the rows to where LargeChange is TRUE, we now get a table with the most significant changes and the rows they start on.

```{r}

# Filtering out large changes, CohenD >= 2
table2 <- table %>%
  mutate(
      LargeChange = case_when(
        CohenD >= 2 ~ TRUE,
        TRUE ~ FALSE
      ),
      Start_Date = df$DateTime[table$begin],
      End_Date = df$DateTime[table$end]
    ) %>%
  filter(
    LargeChange 
  )
table2
```

Using the 'begin' column, the data can be cut and binned between each change point to form individual regimes.

```{r}
 # Assign regime change points
cpts <- table2$begin
        
# Create intervals for regimes  using change points
intervals <- c(0, cpts, Inf)
        
# Create labels for intervals
regime_labels <- seq(from = 1, to = (length(intervals)-1), by = 1)
      
      
# Bin original data based on intervals  
bins <- cut(x = 1:length(df[['constraint_flow']]), breaks = intervals, 
                labels= regime_labels, 
                include.lowest = TRUE, right = FALSE  )
      
# Concat bins to original data
df$Regime <- bins
```

#### Final Results

Plotting the data set and assigning color based on regime shows us the final result.

```{r}
df %>%
  ggplot() + geom_line(aes(x = DateTime, y = constraint_flow, color = Regime)) + geom_line(aes(x = DateTime, y = rate_b), color = 'orange') + labs(x = 'Date', y = 'Constraint Flow')
```

With the data binned, summary statistics can be calculated for each regime.

```{r}
df %>% 
  group_by(Regime) %>%
  summarise("Regime Start" = min(DateTime), "Regime End" = max(DateTime), "Mean Constraint Flow" = mean(constraint_flow), "Standard Deviation" = sd(constraint_flow))
```

The method was successful in identifying the regime change that was suspecting earlier in this analysis. However, it also picked up a third regime where constraint_flow is equal to 0. This is likely a measurement error and can be ignored.

# Simulating A Real World Use Case

To simulate how this can be deployed in the real world, a function was created that contains all the steps of the previous workflow. Starting with one year of data, one month will be added at a time right when the regime change happens, simulating the model picking up the change as it would have happened in the past.

```{r}
online_constraint_flow <- function(df){

# Run online_cp function on df with threshold set to 0.98
ocp <- online_cp(df$constraint_flow, th_cp = 0.98)

# Assigning results to ocp_summary
ocp_summary <- summary(ocp)

# Converting results table to a dataframe
table <- as.data.frame(ocp_summary$result[[2]])

# Calculating Cohen's D and time in Months for each segment
table <- table %>%
    mutate(
      Months = (as.numeric(as.duration(df$DateTime[table$end] -  df$DateTime[table$begin]) / ddays(1)))/30.44,
      CohenD = (abs(lag(mean) - mean)) / sqrt((SD^2 + lag(SD)^2)/2)
    )
# Filtering out large changes, CohenD >= 2
table2 <- table %>%
  mutate(
      LargeChange = case_when(
        CohenD >= 2 ~ TRUE,
        TRUE ~ FALSE
      ),
      Start_Date = df$DateTime[table$begin],
      End_Date = df$DateTime[table$end]
    ) %>%
  filter(
    LargeChange 
  )

# Assign regime change points
cpts <- table2$begin
        
# Create intervals for regimes  using change points
intervals <- c(0, cpts, Inf)
        
# Create labels for intervals
regime_labels <- seq(from = 1, to = (length(intervals)-1), by = 1)
      
      
# Bin original data based on intervals  
bins <- cut(x = 1:length(df[['constraint_flow']]), breaks = intervals, 
                labels= regime_labels, 
                include.lowest = TRUE, right = FALSE  )
      
# Concat bins to original data
df$Regime <- bins

# Plot results
plot = df %>%
  ggplot() + geom_line(aes(x = DateTime, y = constraint_flow, color = Regime)) + geom_line(aes(x = DateTime, y = rate_b), color = 'orange') + labs(x = 'Date', y = 'Constraint Flow')

# Create summaty of regimes
summary_table <- df %>% 
  group_by(Regime) %>%
  summarise("Regime Start" = min(DateTime), "Regime End" = max(DateTime), "Mean Constraint Flow" = mean(constraint_flow), "Standard Deviation" = sd(constraint_flow))
   
# Return list of outputs
return(list(plot = plot, summary = summary_table))
      
}
```

#### Running the model from January 1, 2021 to January 1, 2022

Running the model on this year of data returns only one regime as expected since the change hasn't started happening.

```{r}
df2 = df %>%
  filter(
    DateTime <= '2022-01-01'
  )
```

```{r, results= 'hide'}
result <- online_constraint_flow(df2)
```

```{r}
result$plot

result$summary
```

#### Running the model from January 1, 2021 to February 1, 2022

We see an upwards trend in constraint_flow, but it has not persisted long enough for it to be classified as a regime change. The client specified that this was actually desired because of the possibility of short blips happening. Short blips can be remedied until they come back down.

```{r}
df2 = df %>%
  filter(
    DateTime <= '2022-02-01'
  )
```

```{r, results='hide'}
result <- online_constraint_flow(df2)
```

```{r}
result$plot

result$summary
```

#### Running the model from January 1, 2021 to March 1, 2022

Finally with the entire month of February processed, the model has identified a regime change. The model will also slightly back track and flag the change where it actually started in January, allowing our client to get a clear picture of when it occurred.

```{r}
df2 = df %>%
  filter(
    DateTime <= '2022-03-01'
  )
```

```{r, results='hide'}
result <- online_constraint_flow(df2)
```

```{r}
result$plot

result$summary
```

#### Suggested Actions

With a clear indication that regime change is occurring, the utilities company can now work towards enhancing the stability of the appropriate power grid to accommodate this new-normal of power flow.
