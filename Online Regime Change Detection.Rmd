---
title: "Online Regime Change Detection"
author: "Kyle Pelham, kyleplhm@gmail.com"
date: "2023-03-29"
output: 
  html_document: 
    theme: cerulean
    df_print: kable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

```{r}
library(tidyverse)
library(here)
library(ggplot2)
library(onlineBcp)
library(lubridate)
library(plotly)
library(gt)
```

# Introduction

This analysis is my contribution to a larger Cal Poly MSBA captstone project in support of a large North American Utilities company. Our group was tasked with detecting significant changes in constraint flow beyond normal seasonality also known as **regime changes.** This model leverages an R package that uses Bayesian probabilities to detect changes in time-series data in real-time( a.k.a "online").

The data used in this demonstration is an extract of historical provided by the client for a single constraint. Any geographical labels or names have been removed to maintain confidentiality. Only the knitted html is available at the request of the client.

#### What is a constraint? What is constraint flow? Why is this important?

In the context of a utilities company, constraints refer to limitations in the power grid, impacting its efficiency and reliability. These constraints can arise from transmission line capacity, generation limitations, network topology, and regulatory or market restrictions. When combined with the term "constraint flow," it refers to the flow of electricity through a constrained part of the power grid, which can be influenced by these factors. Power companies and grid operators work to mitigate these constraints, ensuring stable and cost-effective electricity delivery.

Having a method to detect large changes in constraint flow can allow the utilities company to take proactive actions to maintain stability within the power grid instead of reacting when things begin to breakdown. If this change persists, then it is considered a regime change and the utilities company can work towards stabilizing the higher demand for power.

#### Approach:

1.  Import & clean the constraint data

2.  Exploratory analysis

3.  Create baseline with OnlineBcp package

4.  Perform post-processing on OnlineBcp output to determine regime change

5.  Visualize results and suggest appropriate actions

# Import & Clean the Constraint Data

#### Data Dictionary:

-   DateTime: Date and time of observation. Each observation is one hour

-   OpYear: Year observation occurs in

-   OpMonth: Year + month observation occurs in

-   constraint_flow: Flow of electricity within constraint in megawatt hours (MWh)

-   rate_b: The intended upper limit on constraint flow at that given time

-   constraint_flow_ratio: Ratio of constraint flow over rate_b (e.g., constraint_flow/rate_b)

-   shadow_price: cost of intervening when the power constraint is near its limit. e.g., constraint_flow is approaching rate_b

#### Data Cleaning

Reading in the data and using the summary() function shows us that we have a consistent number of NA values across the rate_b, constraint_flow, and constraint_flow_ratio. This is due to those rows being recorded only once per day despite having rows for every hour of the day.

```{r}
df  <- read_csv(here('Data','constraint.csv'), guess_max = 100000)

summary(df)

```

We can safely remove those rows by filtering where constraint_flow is not NA. DateTime is also converted from a string to a DateTime format.

```{r}
df = df %>%
    filter(
      !(is.na(constraint_flow))
    ) 

df$DateTime = as.POSIXct(df$DateTime, format="%m/%d/%Y %H:%M")

summary(df)
```

Using the summary() function again we can see that those rows have been dropped. However, there are still many NA values under shadow_price. This is due to shadow price only being recorded when constraint_flow is near its capacity (rate_b), requiring manual intervention. When this happens, it is referred to as a 'binding_hour'.

This is not necessary for this model, but it is important to track for our client. The dummy variable binding_hour was created to determine when this sequence of events happens. NA rows in shadow_price were also replaced with 0.

Finally, all NA values have been removed and each column is in the correct format.

```{r}

df = df %>%
    mutate(
      binding_hour = factor(case_when(
        is.na(shadow_price) ~ 0,
        TRUE ~ 1
      )),
      shadow_price = case_when(
        is.na(shadow_price) ~ 0,
        TRUE ~ shadow_price
      )
    )

summary(df)

head(df, n=10)
```

# Exploring the Constraint Data

Since this is all historical data, we will need to simulate how this would work in real time. First, we need to find a regime change in the past that we will try and detect. Lets look at a plot of the constraint time-series data with constraint_flow on the y-axis and DateTime on the x axis.

In the graph below, the blue line is the constraint_flow variable and the orange line is rate_b. Remember, rate_b is the upper threshold of constraint_flow before problems arise.

```{r}
df %>%
  ggplot() + geom_line(aes(x = DateTime, y = constraint_flow), color = 'blue') + geom_line(aes(x = DateTime, y = rate_b), color = 'orange') + labs(x = 'Date', y = 'Constraint Flow')
```

Right at the start of 2022, there appears to be a huge shift in constraint flow where it is riding right up to and beyond the rate_b limit. This is indicative of a regime change and is likely costing our client money to keep the grid stable.

This is a good example of what this model is looking to detect, so this is the time period that will be simulated. We don't need all of the data prior to the expected change so we will progress with only data from 2021 and beyond

```{r}
df = df %>%
  filter(
    DateTime >= '2021-01-01'
  )
```

# Online Change Point Detection

```{r}
ocp <- online_cp(df$constraint_flow, th_cp = 0.98)

ocp_summary <- summary(ocp)

plot(ocp_summary)
```

```{r}
 table <- as.data.frame(ocp_summary$result[[2]])
 
 table <- table %>%
    mutate(
      Months = (as.numeric(as.duration(df$DateTime[table$end] - df$DateTime[table$begin]) / ddays(1)))/30.44,
      CohenD = (abs(lag(mean) - mean)) / sqrt((SD^2 + lag(SD)^2)/2)
    )
 
table
```

```{r}
table2 <- table %>%
  mutate(
      LargeChange = case_when(
        CohenD >= 2 ~ TRUE,
        TRUE ~ FALSE
      ),
      Start_Date = df$DateTime[table$begin],
      End_Date = df$DateTime[table$end]
    ) %>%
  filter(
    LargeChange 
  )
table2
```
